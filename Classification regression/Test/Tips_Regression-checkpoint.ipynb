{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d97953-bede-483a-8d9e-09197d3f2978",
   "metadata": {},
   "source": [
    "# <strong>_Exercice 2 (Régression):_</strong>\n",
    " \n",
    "Approximation de la colonne tip  dans la base d'exemples Tips en utilisant le modèle d'arbre de décision CART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aed156bc-e5d9-4019-a2b7-1f0929087d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b81cd-6974-4739-be90-e25d9554e5c2",
   "metadata": {},
   "source": [
    "# Fonction <strong><span style='color: black ;background-color: pink;'>_compute_avg_leaf_value(Y):_ </span></strong>\n",
    "Cette fonction calcule la valeur moyenne de la variable cible Y, ce qui représente la valeur du nœud feuille dans l'arbre de décision. Elle est utilisée pour déterminer la valeur prédite lorsque l'arbre atteint une feuille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faeaa046-f7f8-46d8-9f15-37f44020a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_leaf_value(Y):\n",
    "    return np.mean(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af17fc-3394-4ab6-a977-f34568a1eef7",
   "metadata": {},
   "source": [
    "## Fonction <strong><span style='color: black ;background-color: pink;'>find_optimal_partition(dataset, num_samples, num_features, min_samples, depth_max): </span></strong>\n",
    "Cette fonction trouve le meilleur point de séparation dans le jeu de données en fonction du gain d'information. Elle parcourt toutes les caractéristiques et leurs seuils possibles pour trouver la séparation qui maximise la réduction de variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d8a4312-d995-4d0b-b120-0f7edf28e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_partition(dataset, num_samples, num_features, min_samples, depth_max):\n",
    "    optimal_partition = {}\n",
    "    max_var_reduction = -float(\"inf\") # Initialise la réduction de variance maximale à moins l'infini\n",
    "\n",
    "    # Parcours de toutes les caractéristiques\n",
    "    for feature_idx in range(num_features):\n",
    "        feature_values = dataset[:, feature_idx]\n",
    "        possible_splits = np.unique(feature_values)\n",
    "\n",
    "        ## Parcours de toutes les valeurs uniques de la caractéristique\n",
    "        for split_threshold in possible_splits:\n",
    "             # Divise l'ensemble de données en fonction de la valeur actuelle de la caractéristique\n",
    "            left_data, right_data = perform_split(dataset, feature_idx, split_threshold)\n",
    "\n",
    "            if len(left_data) > 0 and len(right_data) > 0:  # Assure que la division a donné des ensembles de données gauche et droit non vides\n",
    "                 # Calcul de la réduction de variance après la division\n",
    "                y, left_y, right_y = dataset[:, -1], left_data[:, -1], right_data[:, -1]\n",
    "                var_reduction = calculate_variance_reduction(y, left_y, right_y)\n",
    "                 # Met à jour les détails de la partition optimale si la réduction de variance est plus élevée\n",
    "                if var_reduction > max_var_reduction:\n",
    "                    optimal_partition[\"feature_idx\"] = feature_idx\n",
    "                    optimal_partition[\"threshold\"] = split_threshold\n",
    "                    optimal_partition[\"left_data\"] = left_data\n",
    "                    optimal_partition[\"right_data\"] = right_data\n",
    "                    optimal_partition[\"var_reduction\"] = var_reduction\n",
    "                    max_var_reduction = var_reduction # Met à jour la réduction de variance maximale\n",
    "\n",
    "    return optimal_partition # Renvoie la meilleure partition trouvée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38139ed3-a81d-4831-82f2-36bd61ec8db1",
   "metadata": {},
   "source": [
    "# Fonction <strong><span style='color: black ;background-color: pink;'>_perform_split(dataset, feature_idx, threshold)_ </span></strong>\n",
    "Cette fonction divise le jeu de données en deux sous-ensembles en fonction d'une caractéristique et d'un seuil donnés. Elle crée un sous-ensemble dataset_left contenant les lignes où la valeur de la caractéristique est inférieure ou égale au seuil, et un sous-ensemble dataset_right contenant les lignes où la valeur de la caractéristique est supérieure au seuil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92eaf44c-19ed-4065-9ae5-00b7e744616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_split(dataset, feature_idx, threshold):\n",
    "    # Sépare l'ensemble de données en deux sous-ensembles en fonction de la caractéristique et du seuil donnés\n",
    "    left_subset = np.array([row for row in dataset if row[feature_idx] <= threshold])  # Crée le sous-ensemble gauche\n",
    "    right_subset = np.array([row for row in dataset if row[feature_idx] > threshold]) # Crée le sous-ensemble droit\n",
    "    return left_subset, right_subset  # Renvoie les sous-ensembles gauche et droit après la division\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f9ce3-4896-4d1b-8fa9-c0e26df793db",
   "metadata": {},
   "source": [
    "# Fonction <strong><span style='color: black ;background-color: pink;'>_calculate_variance_reduction(parent, left_child, right_child):_ </span></strong>\n",
    "Cette fonction calcule la réduction de variance, qui est une mesure de la réduction de la variance de la variable cible après la séparation. Elle prend en entrée le jeu de données parent et ses sous-ensembles gauche et droit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea1e8b20-e97e-453e-bf50-d1e904345434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_reduction(parent, left_child, right_child):\n",
    "    # Calcule la réduction de la variance résultant de la division des données\n",
    "    weight_l = len(left_child) / len(parent)\n",
    "    weight_r = len(right_child) / len(parent)\n",
    "    # Calcule la réduction de la variance en soustrayant la somme pondérée des variances des enfants de la variance du parent\n",
    "    reduction = np.var(parent) - (weight_l * np.var(left_child) + weight_r * np.var(right_child))\n",
    "    return reduction  # Renvoie la réduction de la variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96981d-ba3a-4231-9da1-3392566e70c2",
   "metadata": {},
   "source": [
    "## Fonction <strong><span style='color: black ;background-color: pink;'>_construct_decision_tree(dataset, min_samples, depth_max, curr_depth=0):_ </span></strong>\n",
    " Cette fonction récursive construit l'arbre de décision. Elle vérifie les conditions d'arrêt (nombre minimum d'échantillons et profondeur maximale) et si elles ne sont pas satisfaites, elle trouve la meilleure séparation, crée des sous-arbres gauche et droit, et renvoie un nœud de décision. Si les conditions d'arrêt sont satisfaites, elle calcule la valeur du nœud feuille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc570df5-800e-4e3f-a654-8ad85cfa35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_decision_tree(dataset, min_samples, depth_max, curr_depth=0):\n",
    "    X, Y = dataset[:, :-1], dataset[:, -1]\n",
    "    num_samples, num_features = np.shape(X)\n",
    "    optimal_partition = {}\n",
    "\n",
    "    if num_samples >= min_samples and curr_depth <= depth_max:\n",
    "         # Trouve la partition optimale pour diviser les données\n",
    "        optimal_partition = find_optimal_partition(dataset, num_samples, num_features, min_samples, depth_max)\n",
    "\n",
    "        if optimal_partition and optimal_partition[\"var_reduction\"] > 0:  # Vérifie si une partition optimale a été trouvée et si la réduction de variance est positive\n",
    "            # Construction récursive des branches gauche et droite de l'arbre de décision\n",
    "            left_branch = construct_decision_tree(optimal_partition[\"left_data\"], min_samples, depth_max, curr_depth + 1)\n",
    "            right_branch = construct_decision_tree(optimal_partition[\"right_data\"], min_samples, depth_max, curr_depth + 1)\n",
    "\n",
    "            return {\n",
    "                \"feature_idx\": optimal_partition[\"feature_idx\"],\n",
    "                \"threshold\": optimal_partition[\"threshold\"],\n",
    "                \"left\": left_branch,\n",
    "                \"right\": right_branch,\n",
    "                \"var_reduction\": optimal_partition[\"var_reduction\"]\n",
    "            }\n",
    "\n",
    "    return {\"value\": compute_avg_leaf_value(Y)} # Retourne une feuille avec la valeur moyenne des feuilles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cbcce-a689-4481-a196-9f85f8ea456a",
   "metadata": {},
   "source": [
    "# Fonction <strong><span style='color: black ;background-color: pink;'>_predict_single_data(x, tree)_ </span></strong>\n",
    "Cette fonction prédit la valeur de la variable cible pour un nouveau point de données x en fonction de l'arbre de décision. Elle traverse récursivement l'arbre jusqu'à atteindre un nœud feu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecff97cd-f835-492c-b70c-9249da5bb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_data(x, tree):\n",
    "    # Vérifie si le nœud est une feuille et renvoie sa valeur si c'est le cas\n",
    "    if tree[\"value\"] is not None:\n",
    "        return tree[\"value\"]\n",
    "        # Obtient la valeur de la fonctionnalité pour l'échantillon\n",
    "    feature_val = x[tree[\"feature_idx\"]]\n",
    "     # Détermine la branche à suivre en fonction de la valeur de la fonctionnalité\n",
    "    if feature_val <= tree[\"threshold\"]:\n",
    "        return predict_single_data(x, tree[\"left\"])\n",
    "    else:\n",
    "        return predict_single_data(x, tree[\"right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "846ab810-2760-4d34-b9d3-a14752e04d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_decision_tree(tree=None, indent=\" \"):\n",
    "    if tree is None:\n",
    "        return\n",
    "    if isinstance(tree, dict):\n",
    "        if \"value\" in tree:\n",
    "            print(tree[\"value\"])\n",
    "        else:\n",
    "            print(\"X_\" + str(tree[\"feature_idx\"]), \"<=\", tree[\"threshold\"], \"?\", tree[\"var_reduction\"])\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            display_decision_tree(tree[\"left\"], indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            display_decision_tree(tree[\"right\"], indent + indent)\n",
    "    else:\n",
    "        print(tree)\n",
    "\n",
    "# Function to train the decision tree\n",
    "def train_tree(X, Y, min_samples=2, depth_max=2):\n",
    "    dataset = np.column_stack((X, Y))\n",
    "    return construct_decision_tree(dataset, min_samples, depth_max)\n",
    "\n",
    "# Function to predict using the decision tree\n",
    "def predict_tree(X, tree):\n",
    "    return [predict_single_data(x, tree) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5947fb4-4c94-48ee-a2b3-7179452f0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_0 <= 23.95 ? 0.2953852130775213\n",
      " left:X_0 <= 16.0 ? 0.08998460746712483\n",
      "  left:X_0 <= 10.07 ? 0.014016035633128254\n",
      "    left:X_1 <= 1.0 ? 0.04000000000000001\n",
      "        left:1.3333333333333333\n",
      "        right:1.8333333333333333\n",
      "    right:X_0 <= 10.33 ? 0.0024681892034978253\n",
      "        left:2.2\n",
      "        right:2.017543859649123\n",
      "  right:X_1 <= 2.45 ? 0.05725181103968935\n",
      "    left:X_4 <= Sun ? 0.18750000000000006\n",
      "        left:3.3333333333333335\n",
      "        right:2.3333333333333335\n",
      "    right:X_1 <= 4.3 ? 0.049554183813442676\n",
      "        left:2.4\n",
      "        right:3.25\n",
      " right:X_3 <= No ? 0.2962799295618467\n",
      "  left:X_0 <= 26.41 ? 0.1950342562708791\n",
      "    left:X_0 <= 25.29 ? 0.2232142857142856\n",
      "        left:3.4285714285714284\n",
      "        right:2.0\n",
      "    right:X_4 <= Sun ? 0.3448979591836734\n",
      "        left:3.8666666666666667\n",
      "        right:5.166666666666667\n",
      "  right:X_0 <= 34.63 ? 0.1367134755603563\n",
      "    left:X_0 <= 30.46 ? 0.17755681818181834\n",
      "        left:2.909090909090909\n",
      "        right:2.0\n",
      "    right:X_0 <= 38.73 ? 0.24489795918367352\n",
      "        left:4.0\n",
      "        right:3.0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"tips.csv\")\n",
    "\n",
    "X = data.iloc[:, :-1].values #Extraction des caractéristiques du jeu de données dans la variable \"X\" en sélectionnant toutes les lignes et toutes les colonnes sauf la dernière\n",
    "Y = data.iloc[:, -1].values.reshape(-1, 1) #Extraction de la variable cible du jeu de données dans la variable \"Y\" en sélectionnant toutes les lignes de la dernière colonne et en la remodelant en une colonne.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
    "\n",
    "# Initialize and fit the model\n",
    "decision_tree = train_tree(X_train, Y_train, min_samples=3, depth_max=3)\n",
    "\n",
    "# Affichage de la structure de l'arbre de décision entraîné en utilisant la fonction print_tree.\n",
    "display_decision_tree(decision_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
